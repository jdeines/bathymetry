---
title: "Clean Point Data"
author: "Jill Deines"
date: "Friday, December 05, 2014"
output: 
  html_document:
    toc: yes
---

```{r knitrOpts, echo=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE)
opts_chunk$set(echo=TRUE)
opts_chunk$set(fig.path='figures/cleanpoints/')
opts_chunk$set(cache.path='cache/cleanpoints/')
```

## Overview
**Problem:** our point dataset has ~80,000 co-located points (ended up being 41,461 extra points)
**Goal:** Aggregate all co-located points into one point with a mean lake depth value.
**Output:** Cleaned point datasets for analysis, including various point subsets

We have made several point datasets thus far (different combinations of deep depths, shallow depths, and shoreline points), so I am going to start by cleaning Blaze's master 'Kayak_UM_MSU_2014.shp', which has combined deep depths and shallow points.

We can then use this base copy to re-run code that produces the "All_PointsWithShoreline.shp" shapefile (aka, by fixing the source, we can fix all the products).

Kayla also combined a few datasets into two shallow depth datasets (in case we krige by shelf/deep lake areas) using files that Blaze incorporated into Kayak_UM_MSU_2014.shp, so I will also check those input files for duplicates.

**R Packages Needed*

```{r packages, message=FALSE, echo=TRUE}
library(rgdal)
library(maptools)
```

## Main depth point dataset
Here, I go to town on 'Kayak_UM_MSU_2014.shp'. Note I also create the `lakedepth` field here.

```{r mainPoints, message=FALSE, eval=FALSE}
# load points
gisDir <- 'S:/Projects/2013/Higgins_Lake/Higgins_Bath/GIS'
points <- readOGR(gisDir,"Kayak_UM_MSU_2014", verbose=F)

# identify rows which are duplicate points
dupes <- zerodist(points)             # check for points on top of each other
dupvec <- c(dupes[,1],dupes[,2])         
rowsOfInterest <- unique(dupvec)
length(rowsOfInterest)                # just curious

# extract duplicated coords and aggregate by coordinate (take mean)
dupePoints <- points[rowsOfInterest,] # subset duplicated points
# make a factor that incorporates x,y coords
dupePoints$location <- as.factor(paste0(dupePoints$Lat,dupePoints$Lon)) 
# get Bot_Ele mean for each location
dupedf <- as.data.frame(dupePoints)
reduced <-aggregate(dupedf, by=list(dupedf$location), FUN=mean, na.rm=TRUE)
# clean and spatialize
coordinates(reduced) <- ~coords.x1+coords.x2
reduced <- reduced[,-c(1,6)]

# combine reduced point dataset with original data singletons 
# remove dupes from original data
singlePoints <- points[-rowsOfInterest,]
proj4string(reduced) <- proj4string(singlePoints)
cleanpoints <- spRbind(singlePoints,reduced)

check <- zerodist(cleanpoints)  # 0, yay!

# make lake depth (m) using elevation of shoreline
cleanpoints$lakedepth <- 351.733 - cleanpoints$Bot_Ele

# write out a lat/long and utm file
writeOGR(cleanpoints, paste0(gisDir,'/cleanData_duplicatesRemoved'), 
         "Kayak_UM_MSU_2014_noDupes_latlong", driver = 'ESRI Shapefile')

utmproj <- '+proj=utm +zone=16 +ellps=GRS80 +datum=NAD83 +units=m +no_defs' 
cleanpoints.utm <- spTransform(cleanpoints, CRS(utmproj))
writeOGR(cleanpoints.utm, paste0(gisDir,'/cleanData_duplicatesRemoved'), 
         "Kayak_UM_MSU_2014_noDupes_utm", driver = 'ESRI Shapefile')

# tidy up
rm(points, dupes, dupvec, rowsOfInterest, dupePoints, dupedf, reduced, singlePoints, check, cleanpoints, cleanpoints.utm)
```

Now check for duplicates in the shore points

```{r cleanShore, eval=FALSE}
# load shoreline points
gisDir <- 'S:/Projects/2013/Higgins_Lake/Higgins_Bath/GIS'
shorepoints <- readOGR(gisDir,"Shoreline_Points_GCS", verbose=F)

# match column names between shore points and data points
shorepoints <- shorepoints[,c('Lat','Lon','Bot_Ele','Id')]
names(shorepoints) <- c('Lat','Lon','Bot_Ele','ID')
#add shore depth 
shorepoints$lakedepth <- 0
# reproject
utmproj <- '+proj=utm +zone=16 +ellps=GRS80 +datum=NAD83 +units=m +no_defs' 
shorepoints <- spTransform(shorepoints, CRS(utmproj))

dupes <- zerodist(shorepoints)
dupvec <- c(dupes[,1],dupes[,2])         # make a vector
rowsOfInterest <- unique(dupvec)

# extract duplicated coords and aggregate by coordinate (take mean)
dupePoints <- shorepoints[rowsOfInterest,] # subset duplicated points
# make a factor that incorporates x,y coords
dupePoints$location <- as.factor(paste0(dupePoints$Lat,dupePoints$Lon)) 
# get Bot_Ele mean for each location
dupedf <- as.data.frame(dupePoints)
reduced <-aggregate(dupedf, by=list(dupedf$location), FUN=mean, na.rm=TRUE)
# clean and spatialize
coordinates(reduced) <- ~coords.x1+coords.x2
reduced <- reduced[,-c(1,7)]

# combine reduced point dataset with original data singletons 
# remove dupes from original data
singlePoints <- shorepoints[-rowsOfInterest,]
proj4string(reduced) <- proj4string(singlePoints)
cleanpoints <- spRbind(shorepoints,reduced)

# write a utm file
writeOGR(cleanpoints, paste0(gisDir,'/cleanData_duplicatesRemoved'), 
         "Shoreline_Points_noDupes_utm", driver = 'ESRI Shapefile')

# tidy up
rm(dupes, dupvec, rowsOfInterest, dupePoints, dupedf, reduced, singlePoints, cleanpoints, shorepoints, utmproj)
```


### Re-make All_PointsWithShoreline shapefile

```{r allPointsShoreline, eval=FALSE}
# load files
gisCleanDir <- 'S:/Projects/2013/Higgins_Lake/Higgins_Bath/GIS/cleanData_duplicatesRemoved'
# load shoreline points
shorepoints <- readOGR(gisCleanDir,"Shoreline_Points_noDupes_utm", verbose=F)
# load all clean data
points <- readOGR(gisCleanDir,"Kayak_UM_MSU_2014_noDupes_utm", verbose=F)

# combine shoreline with points
allpoints <- spRbind(points, shorepoints)

# write out Shapefile
writeOGR(allpoints,gisCleanDir,"AllPoints_withShoreline_utm", driver= "ESRI Shapefile")

#tidy up
rm(allpoints, points, shorepoints)
```

## Shallow Depths Only Datasets
Presumably, the Kayaking shapefile also has duplicates, and possibly the shoal file. 

### Check and Clean Duplicates

```{r shoal}
# load points
gisDir <- 'S:/Projects/2013/Higgins_Lake/Higgins_Bath/GIS'
shoal_shore <- readOGR(gisDir,"Corrected_Elev_Merged_Shoal_and_Shore2", verbose=F)

# match names to All Points dataset
shoal_shore <- shoal_shore[,c('Lat', 'Lon','Bot_Ele')]
shoal_shore$ID <- 0

# reproject to utm
utmproj <- '+proj=utm +zone=16 +ellps=GRS80 +datum=NAD83 +units=m +no_defs' 
shoal_shore <- spTransform(shoal_shore, CRS(utmproj))

dupes <- zerodist(shoal_shore)           # check for points on top of each other
dupvec <- c(dupes[,1],dupes[,2])         # make a vector
rowsOfInterest <- unique(dupvec)

# extract duplicated coords and aggregate by coordinate (take mean)
dupePoints <- shoal_shore[rowsOfInterest,] # subset duplicated points
# make a factor that incorporates x,y coords
dupePoints$location <- as.factor(paste0(dupePoints$Lat,dupePoints$Lon)) 
# get Bot_Ele mean for each location
dupedf <- as.data.frame(dupePoints)
reduced <-aggregate(dupedf, by=list(dupedf$location), FUN=mean, na.rm=TRUE)
# clean and spatialize
coordinates(reduced) <- ~coords.x1+coords.x2
reduced <- reduced[,-c(1,6)]

# combine reduced point dataset with original data singletons 
# remove dupes from original data
singlePoints <- shoal_shore[-rowsOfInterest,]
proj4string(reduced) <- proj4string(singlePoints)
cleanpoints <- spRbind(shoal_shore,reduced)

# make lake depth (m) using elevation of shoreline
cleanpoints$lakedepth <- 351.733 - cleanpoints$Bot_Ele

# write a utm file
writeOGR(cleanpoints, paste0(gisDir,'/cleanData_duplicatesRemoved'), 
         "Corrected_Elev_Merged_Shoal_and_Shore2_noDupes_utm", 
         driver = 'ESRI Shapefile')

# tidy up
rm(points, dupes, dupvec, rowsOfInterest, dupePoints, dupedf, reduced, singlePoints, check, cleanpoints, shoal_shore, utmproj)
```


```{r kayak}
# load points
gisDir <- 'S:/Projects/2013/Higgins_Lake/Higgins_Bath/GIS'
kayak <- readOGR(gisDir,"Kayaked2014", verbose=F)

# match names to All Points dataset
kayak$ID <- 0

# reproject to utm
utmproj <- '+proj=utm +zone=16 +ellps=GRS80 +datum=NAD83 +units=m +no_defs' 
kayak <- spTransform(kayak, CRS(utmproj))

dupes <- zerodist(kayak)             # check for points on top of each other
dupvec <- c(dupes[,1],dupes[,2])         # make a vector
rowsOfInterest <- unique(dupvec)

# extract duplicated coords and aggregate by coordinate (take mean)
dupePoints <- kayak[rowsOfInterest,] # subset duplicated points
# make a factor that incorporates x,y coords
dupePoints$location <- as.factor(paste0(dupePoints$Lat,dupePoints$Lon)) 
# get Bot_Ele mean for each location
dupedf <- as.data.frame(dupePoints)
reduced <-aggregate(dupedf, by=list(dupedf$location), FUN=mean, na.rm=TRUE)
# clean and spatialize
coordinates(reduced) <- ~coords.x1+coords.x2
reduced <- reduced[,-c(1,6)]

# combine reduced point dataset with original data singletons 
# remove dupes from original data
singlePoints <- kayak[-rowsOfInterest,]
proj4string(reduced) <- proj4string(singlePoints)
cleanpoints <- spRbind(singlePoints,reduced)

# make lake depth (m) using elevation of shoreline
cleanpoints$lakedepth <- 351.733 - cleanpoints$Bot_Ele

writeOGR(cleanpoints, paste0(gisDir,'/cleanData_duplicatesRemoved'), 
         "Kayaked2014__noDupes_utm", driver = 'ESRI Shapefile')

# tidy up
rm(dupes, dupvec, rowsOfInterest, dupePoints, dupedf, reduced, singlePoints, cleanpoints, kayak, utmproj)
```

### Combine clean points
Make (1) file that combines shoal and kayak, and (2) file that combined shoal, kayak, and shoreline points


```{r combineShallow, eval=FALSE}
# load clean data
gisCleanDir <- 'S:/Projects/2013/Higgins_Lake/Higgins_Bath/GIS/cleanData_duplicatesRemoved'
kayak <- readOGR(gisCleanDir,"Kayaked2014__noDupes_utm", verbose=F)
shoal <- readOGR(gisCleanDir,"Corrected_Elev_Merged_Shoal_and_Shore2_noDupes_utm", verbose=F)
shore <- readOGR(gisCleanDir, "Shoreline_Points_noDupes_utm", verbose=F)

# combine shoreline with points
shallowdata <- spRbind(kayak, shoal)
shallowshore <- spRbind(shallowdata, shore)

writeOGR(shallowdata, gisCleanDir, 'Shoal_Kayak_utm', driver='ESRI Shapefile')
writeOGR(shallowshore, gisCleanDir, 'allShallow_shore_utm', driver='ESRI Shapefile')
```